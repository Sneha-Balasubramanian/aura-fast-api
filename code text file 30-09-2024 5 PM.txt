import cv2
import numpy as np
import streamlit as st
import openai
from openai import OpenAI
import json
import fitz 
from pdf2image import convert_from_bytes

from utils import get_image_description
from confidence import get_confidence_level
import os  # This is the missing import
from dotenv import load_dotenv  # For loading the .env file

# Load the environment variables from the .env file
load_dotenv()
# Access the OpenAI API key from the environment variables
openai.api_key = os.getenv("OPEN_API_KEY")





# Updated prompt for processing and analyzing textual data
detailed_prompt = (
    """
    "You are a highly skilled and detail-oriented assistant specialized in processing and analyzing textual data extracted from PDFs. "
    "Your primary task is to help the user extract specific details, numerical values, or other relevant information from the text content provided. "
    "The user may ask questions or make requests related to various aspects of the extracted text, including counting items, identifying categories, extracting personal details, and more.\n\n"
    "When responding to the user's requests:\n"
    "1. Understand the Context: Carefully read the user's request to ensure you fully understand what information they need and do not extract the information which is not in the prompt\n"
    "2. Text Extraction and Analysis: Analyze the provided text to extract accurate information, such as numerical values, names, categories, or other details.\n"
    "3. Clear and Concise Responses: Provide clear, concise, and accurate responses based on the text content. Include relevant details and context to ensure the user gets the exact information they need.\n"
    "4. Highlight Key Information: When listing items or details, organize them in a structured format (e.g., bullet points or numbered lists) for easy readability.\n"
    "5. Accuracy and Verification: Double-check your analysis to ensure the accuracy of the extracted information, especially when dealing with numerical data or critical details.\n"
    "6. Handle Complex Queries: If the user's query is complex or involves multiple steps, break down the response into logical parts and guide the user through each step.\n"
    "7. Your answers should strictly like an object with key-value pairs. There can be other data structure contain inside the object as well if necessary. Add descriptions if only user asks for it. If user asks for certain values, provide object like structure.\n\n"
    "8. Extract only the particular key-value pair the user is asking for."
    """
)

# Streamlit app layout
st.title("Image Analysis and Description Tool")

# Image Analysis Section
st.write("Upload an image or PDF and get a description using GPT-4.")

# Textbox for updating the prompt
user_prompt = st.text_input("Enter the prompt for image description", "What’s in this image?")

# Upload image or PDF button
uploaded_file = st.file_uploader("Choose an image or PDF...", type=["jpg", "jpeg", "png", "pdf"])

if uploaded_file is not None:
    st.write(f"Uploaded file type: {uploaded_file.type}")
    print('lkjhgfkjtdrsejhgyftdghj',uploaded_file )
    if uploaded_file.type == "application/pdf":
        # Handle PDF file
        st.write("Processing PDF...")
        
        # Open the PDF with PyMuPDF to get page count
        pdf_document = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        page_count = pdf_document.page_count
        st.write(f"PDF has {page_count} pages.")

        if page_count == 1:
            # Single-page PDF: Convert the first page to an image and process it
            images = convert_from_bytes(uploaded_file.getbuffer(), fmt='jpeg')
            img = np.array(images[0])
            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert image to RGB for OpenAI
            
            # Convert image to OpenAI-supported format
            _, img_encoded = cv2.imencode('.jpg', img_rgb)
            img_bytes = img_encoded.tobytes()

            # Get description for the image
            descriptions = []
            descriptions.append(get_image_description(openai, img_bytes, detailed_prompt, user_prompt))

        else:
            # Multi-page PDF: Extract textual data from all pages
            clinet = OpenAI()
            st.write("Extracting text from PDF...")
            all_text = ""
            
            for page_num in range(page_count):
                page = pdf_document.load_page(page_num)
                page_text = page.get_text("text")
                all_text += f"\n--- Page {page_num + 1} ---\n{page_text}"

            # Process the extracted text with OpenAI
            response =  openai.chat.completions.create(
                engine="text-davinci-003",
                prompt=f"Extract information based on the prompt:\n{user_prompt}\nText:\n{all_text}",
                max_tokens=1000
            )
            descriptions = [response.choices[0].message.content]

    else:
        # Read the uploaded image
        img_before = cv2.imdecode(np.frombuffer(uploaded_file.read(), np.uint8), cv2.IMREAD_COLOR)

        if img_before is None:
            st.error("Error loading image. Please upload a valid image file.")
        else:
            # Display the original uploaded image
            st.image(cv2.cvtColor(img_before, cv2.COLOR_BGR2RGB), caption='Uploaded Image', use_column_width=True)

            # Convert to grayscale
            img_gray = cv2.cvtColor(img_before, cv2.COLOR_BGR2GRAY)

            # Edge detection
            img_edges = cv2.Canny(img_gray, 100, 100, apertureSize=3)

            # Check if the image is readable
            if np.var(img_gray) < 500:  # Adjust the threshold as necessary
                st.error("The image is not readable. Please upload an image with better clarity.")
            else:
                st.write("Classifying...")
                descriptions = []

                # Call the function three times and append results to the list
                for _ in range(3):
                    description = get_image_description(openai, uploaded_file.getbuffer(), detailed_prompt, user_prompt)
                    descriptions.append(description)

    description_json = {
        "descriptions": descriptions  # Store the list of descriptions
    }
    st.write(description_json)

    descriptions_str = json.dumps(descriptions)
    result = get_confidence_level(openai, descriptions_str)

    st.write(result)
----------------------------------------------------------------------------
import base64

def get_image_description(client, uploaded_file, detailed_prompt, user_prompt):
    # Encode the uploaded image in base64
    encoded_image = base64.b64encode(uploaded_file).decode('utf-8')

    # Create the GPT-4 API request
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "system",
                "content": [
                    {
                        "type": "text",
                        "text": f"{detailed_prompt} Please respond specifically to the user's prompt in JSON format: {user_prompt}."
                    },
                ]
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/png;base64,{encoded_image}"}
                    },
                ],
            }
        ],
        max_tokens=1500,
        temperature=0.0,
        response_format={"type": "json_object"}  # Turn on JSON mode
    )

    # Extract and return the description
    return response.choices[0].message.content
--------------------------------------------------------------------------
def get_confidence_level(client, descriptions):
    # Create the GPT-4o API request
    response = client.chat.completions.create(
        model="gpt-4o-mini",
    
        messages=[
            {
                "role": "system",
                "content": 
                """
                    you are a helpful AI assistant specialized in analyzing all the key-value pairs given as input and evaluating the confidence score. I have done three API calls which will also give you the output three times, the result of which is appended in a list (descriptions).

                    Follow the following steps:

                    1. The inputs will be key-value pairs; I want you to analyze them completely.
                    2. Select the most accurate key-value pair for all the fields  among the entries provided.
                    3. For every field's selected key-value pair, evaluate the confidence score.
                    4. Give the output in the following format:
                    "key-value pair", "confidence score": x, where the x value ranges between (0 to 1).
                    5. Don't print the whole process , give only the final result 
                    6. Evaluation of the confidence score is based on the consistency of the outputs. Compare the outputs in all of the jsons, Higher the consistency, the higher the confidence score is.
                """
            },
            {
                "role": "user",
                "content": descriptions
            },
        ],
        max_tokens=1500,
        temperature=0.0
         
    )
    return response.choices[0].message.content


======================================================================================
Pseudo code-----------------
import necessary libraries

load environment variables for API key

define detailed prompt for OpenAI

initialize Streamlit app with title and instructions

on file upload:
    if file is a PDF:
        if PDF contains images:
            initialize empty list for extracted_images
            for each page in PDF:
                convert page to image and store in a variable (maintain existing image conversion format)
                append converted image to extracted_images
                extract information from the image (handle encoding/decoding as needed)
        else:  // PDF contains textual data
            initialize empty list for extracted_text
            for each page in PDF:
                extract text and append to extracted_text
                // Check for errors during text extraction
                if error occurs:
                    log error message and continue to next page
                encode/decode text if necessary

        // Analyze the extracted information
        if extracted_images is not empty:
            analyze extracted_images using OpenAI
        if extracted_text is not empty:
            analyze extracted_text using OpenAI
            
    else if file is an image:
        load and process the image
        check if image is readable
        if readable:
            initialize empty list for image_descriptions
            for i from 1 to 3:  // Generate multiple descriptions
                description = get image description using OpenAI
                append description to image_descriptions

// Create JSON structure for descriptions and extracted data
initialize json_structure with descriptions and any extracted data

// Evaluate confidence levels for descriptions
evaluate confidence level of json_structure using OpenAI

// Display results in Streamlit app
render results in user-friendly format
-------------------------------------------------------------------------------------
if uploaded_file is not None:
    st.write(f"Uploaded file type: {uploaded_file.type}")

    descriptions = []  # Initialize the descriptions list here

    if uploaded_file.type == "application/pdf":
        # Handle PDF file
        st.write("Processing PDF...")

        # Open the PDF with PyMuPDF to get page count
        pdf_document = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        page_count = pdf_document.page_count
        st.write(f"PDF has {page_count} pages.")

        if page_count == 1:
            # Single-page PDF: Convert the first page to an image and process it
            images = convert_from_bytes(uploaded_file.getbuffer(), fmt='jpeg')
            img = np.array(images[0])
            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

            # Convert image to OpenAI-supported format
            _, img_encoded = cv2.imencode('.jpg', img_rgb)
            img_bytes = img_encoded.tobytes()

            # Get description for the image
            descriptions.append(get_image_description(openai, img_bytes, detailed_prompt, user_prompt))

        else:
            # Multi-page PDF: Extract textual data from all pages
            extracted_images = []
            extracted_text = []
            st.write("Extracting data from PDF...")

            for page_num in range(page_count):
                page = pdf_document.load_page(page_num)

                # Check if the page contains an image
                if page.get_text("text") == "":
                    # If the page is an image, convert it to an image format
                    images = convert_from_bytes(uploaded_file.getbuffer(), fmt='jpeg', first_page=page_num + 1, last_page=page_num + 1)
                    img = np.array(images[0])
                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    _, img_encoded = cv2.imencode('.jpg', img_rgb)
                    img_bytes = img_encoded.tobytes()
                    extracted_images.append(img_bytes)
                    # Extract information from the image
                    image_info = get_image_description(openai, img_bytes, detailed_prompt, user_prompt)
                    descriptions.append(image_info)  # Add the image information to descriptions
                else:
                    # Extract text from the page
                    page_text = page.get_text("text")
                    extracted_text.append(page_text)

                    # Optionally handle encoding/decoding if needed

            # Analyze the extracted images and text
            if extracted_images:
                for img_bytes in extracted_images:
                    descriptions.append(get_image_description(openai, img_bytes, detailed_prompt, user_prompt))

            if extracted_text:
                all_text = "\n".join(extracted_text)
                response = openai.chat.completions.create(
                    engine="text-davinci-003",
                    prompt=f"Extract information based on the prompt:\n{user_prompt}\nText:\n{all_text}",
                    max_tokens=1000
                )
                descriptions.append(response.choices[0].message.content)

    else:
        # Read the uploaded image
        img_before = cv2.imdecode(np.frombuffer(uploaded_file.read(), np.uint8), cv2.IMREAD_COLOR)

        if img_before is None:
            st.error("Error loading image. Please upload a valid image file.")
        else:
            # Display the original uploaded image
            st.image(cv2.cvtColor(img_before, cv2.COLOR_BGR2RGB), caption='Uploaded Image', use_column_width=True)

            # Check if the image is readable
            if np.var(cv2.cvtColor(img_before, cv2.COLOR_BGR2GRAY)) < 500:  # Adjust the threshold as necessary
                st.error("The image is not readable. Please upload an image with better clarity.")
            else:
                st.write("Classifying...")
                
                # Call the function three times and append results to the list
                for _ in range(3):
                    description = get_image_description(openai, uploaded_file.getbuffer(), detailed_prompt, user_prompt)
                    descriptions.append(description)

    # Create JSON structure for descriptions
    description_json = {
        "descriptions": descriptions  # Store the list of descriptions
    }
    st.write(description_json)

    descriptions_str = json.dumps(descriptions)
    result = get_confidence_level(openai, descriptions_str)

    st.write(result)




===================================================================================================================================
===================================================================================================================================
date 01-10-2024 code can extract pdf which has text, pdf whith images in it and images as well

import cv2
import numpy as np
import streamlit as st
import openai
from openai import OpenAI
import json
import fitz 
from pdf2image import convert_from_bytes

from utils import get_image_description
from confidence import get_confidence_level
import os  # This is the missing import
from dotenv import load_dotenv  # For loading the .env file

# Load the environment variables from the .env file
load_dotenv()
# Access the OpenAI API key from the environment variables
openai.api_key = os.getenv("OPEN_API_KEY")





# Updated prompt for processing and analyzing textual data
detailed_prompt = (
    """
    "You are a highly skilled and detail-oriented assistant specialized in processing and analyzing textual data extracted from PDFs. "
    "Your primary task is to help the user extract specific details, numerical values, or other relevant information from the text content provided. "
    "The user may ask questions or make requests related to various aspects of the extracted text, including counting items, identifying categories, extracting personal details, and more.\n\n"
    "When responding to the user's requests:\n"
    "1. Understand the Context: Carefully read the user's request to ensure you fully understand what information they need and do not extract the information which is not in the prompt\n"
    "2. Text Extraction and Analysis: Analyze the provided text to extract accurate information, such as numerical values, names, categories, or other details.\n"
    "3. Clear and Concise Responses: Provide clear, concise, and accurate responses based on the text content. Include relevant details and context to ensure the user gets the exact information they need.\n"
    "4. Highlight Key Information: When listing items or details, organize them in a structured format (e.g., bullet points or numbered lists) for easy readability.\n"
    "5. Accuracy and Verification: Double-check your analysis to ensure the accuracy of the extracted information, especially when dealing with numerical data or critical details.\n"
    "6. Handle Complex Queries: If the user's query is complex or involves multiple steps, break down the response into logical parts and guide the user through each step.\n"
    "7. Your answers should strictly like an object with key-value pairs. There can be other data structure contain inside the object as well if necessary. Add descriptions if only user asks for it. If user asks for certain values, provide object like structure.\n\n"
    "8. Extract only the particular key-value pair the user is asking for."
    """
)

# Streamlit app layout
st.title("PDF/Image Analysis Tool")

# Image Analysis Section
st.write("Upload an image or PDF and get a description using GPT-4o.")

# Textbox for updating the prompt
user_prompt = st.text_input("Enter the prompt for image description", "What’s in the documnet?")

# Upload image or PDF button
uploaded_file = st.file_uploader("Choose an image or PDF...", type=["jpg", "jpeg", "png", "pdf"])

# Function to extract text from PDF using PyMuPDF (fitz)
def extract_pdf_text(uploaded_file):
    """Extract text from a PDF file using PyMuPDF."""
    text = ""
    with fitz.open(stream=uploaded_file.read(), filetype="pdf") as doc:
        for page in doc:
            text += page.get_text()
    return text

# Function to check if the PDF contains images using PyMuPDF and pdf2image
def has_images_in_pdf(uploaded_file):
    """
    Check if a PDF contains images and verify if they are in typical image modes (RGB/RGBA).
    """
    # Step 1: Check if the PDF contains images using PyMuPDF (fitz)
    pdf_document = fitz.open(stream=uploaded_file.read(), filetype="pdf")
    has_images = False

    for page in pdf_document:
        images = page.get_images(full=True)
        if images:
            has_images = True
            break
    
    uploaded_file.seek(0)  # Reset the file pointer

    if not has_images:
        return False  # No images found

    # Step 2: If images were found, check their modes using pdf2image
    pdf_images = convert_from_bytes(uploaded_file.getbuffer())
    
    for image in pdf_images:
        if image.mode in ["RGB", "RGBA"]:  # Typical image modes
            return True  # Images found in acceptable modes
    
    return False  # No images with acceptable modes

# Function to process and convert PDF pages with images into image descriptions
def process_pdf_images(uploaded_file):
    """Convert PDF pages to images and get descriptions using GPT."""
    images = convert_from_bytes(uploaded_file.getbuffer(), fmt='jpeg')
    descriptions = []
    
    for img in images:
        img = np.array(img)
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert image to RGB
        
        # Convert image to OpenAI-supported format
        _, img_encoded = cv2.imencode('.jpg', img_rgb)
        img_bytes = img_encoded.tobytes()

        # Get description for each image
        description = get_image_description(openai, img_bytes, detailed_prompt, user_prompt)
        descriptions.append(description)
    
    return descriptions

# Main processing logic
if uploaded_file is not None:
    if uploaded_file.type == "application/pdf":
        st.write("Processing PDF...")

        # Check if the PDF contains images in acceptable modes
        if has_images_in_pdf(uploaded_file):
            st.write("Images found in the PDF. Processing images...")

            # Convert PDF pages with images to image format and get descriptions
            descriptions = process_pdf_images(uploaded_file)

            # Display image descriptions
            st.write("Image descriptions from the PDF:")
            st.write(descriptions)

            # Get confidence level for the descriptions
            descriptions_str = json.dumps(descriptions)
            confidence_result = get_confidence_level(openai, descriptions_str)
            st.write(confidence_result)

        else:
            st.write("No images found in the PDF or no valid images. Extracting text...")

            # Extract text from PDF
            pdf_text = extract_pdf_text(uploaded_file)

            if pdf_text.strip():  # If text is found
                st.write("Extracted text from the PDF:")
                st.write(pdf_text)
            else:
                st.write("No text found in the PDF.")
    else:
        # Handle image file (JPEG/PNG)
        img_before = cv2.imdecode(np.frombuffer(uploaded_file.read(), np.uint8), cv2.IMREAD_COLOR)

        if img_before is None:
            st.error("Error loading image. Please upload a valid image file.")
        else:
            st.image(cv2.cvtColor(img_before, cv2.COLOR_BGR2RGB), caption='Uploaded Image', use_column_width=True)
            img_gray = cv2.cvtColor(img_before, cv2.COLOR_BGR2GRAY)
            img_edges = cv2.Canny(img_gray, 100, 100, apertureSize=3)

            if np.var(img_gray) < 500:  # Adjust the threshold as necessary
                st.error("The image is not readable. Please upload a clearer image.")
            else:
                st.write("Classifying...")
                descriptions = []

                for _ in range(3):
                    description = get_image_description(openai, uploaded_file.getbuffer(), detailed_prompt, user_prompt)
                    descriptions.append(description)

                st.write(descriptions)

                descriptions_str = json.dumps(descriptions)
                result = get_confidence_level(openai, descriptions_str)
                st.write(result)
                
================================================================================================================================
        else:
            st.write("No images found in the PDF or no valid images. Extracting text...")

            # Extract text from PDF and collect all pages into a list
            all_text_descriptions = extract_pdf_text(uploaded_file)

            if all_text_descriptions:  # Check if there are any extracted texts
                st.write("Extracted text from the PDF:")
                st.write(all_text_descriptions)

                # Convert the list of text descriptions to JSON for confidence analysis
                text_descriptions_str = json.dumps(all_text_descriptions)
                confidence_result = get_confidence_level(openai, text_descriptions_str)
                st.write(confidence_result)
            else:
                st.write("No text found in the PDF.")

----------------------------
import base64

def get_image_description(client, uploaded_file, system_prompt, user_prompt):
    # Encode the uploaded image in base64
    
    if isinstance(uploaded_file, list): 
        encoded_image_list = [get_image_content(base64.b64encode(i).decode('utf-8')) for i in uploaded_file]
    else: 
        encoded_image_list = [get_image_content(base64.b64encode(uploaded_file).decode('utf-8'))]

    # Create the GPT-4 API request
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "system",
                "content": [
                    {
                        "type": "text",
                        "text": system_prompt
                    },
                ]
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": f"User Prompt: {user_prompt}"
                    }
                ],
            },
            {
                "role": "user",
                "content": [
                    *encoded_image_list
                ],
            }
        ],
        max_tokens=1500,
        temperature=0.3,
        response_format={"type": "json_object"}  # Turn on JSON mode
    )

    # Extract and return the description
    return response.choices[0].message.content


def get_image_content(image_content):
    return {
                "type": "image_url",
                "image_url": {"url": f"data:image/png;base64,{image_content}"}
            }